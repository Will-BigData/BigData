Creating a RDD different ways 

1)rdd1 = sc.parallelize(["jan","feb"])
type(rdd1)
rdd1.collect()

Transformation

2)words = sc.parallelize(["ja","feb","kiojh"])
wordpair = words.map(lambda w: (w[0], w))
wordpair.collect()

3)a = sc.parallelize(["dog","almon"])
a.getNumPartitions()
a1=a.repartion(2)
a1.getNumPartitions()
a1.coalesce(1)
a1.getNumPartitions()
b = a1.map(lambda x: len(x))
b.collect()
c = a1.zip(b)
c.collect()

4)a = sc.parallelize(range(1,10))
b = a.filter(lambda x: x%2 ==0)
b.collect()

5)a.groupBy(lambda x: "even" if x%2 ==0  else "odd" ).mapValues(list).collect()

6)a = sc.parallelize(["hi","hello","world"])
b = a.keyBy(lambda x: len(x))
b.collect()
c = sc.parallelize(["hi","hello","koil"])
d = c.keyBy(len)
d.collect()
k = b.join(d)
k.collect()
k.toDebugString

7)
textFile = sc.textFile("/user/will/people.txt")
textFile.map(lambda line: line.split("\t")).collect()
textFile.flatMap(lambda line: line.split("\t")).collect()

Action
1) a = sc.parallelize(range(1,11))
b = a.reduce(lambda x,y: x+y)
b
type(b)

2) b = sc.parallelize(["hi","hello","hii","gbhyu","youtu","cat","well","hell","anitha"])
b.takeOrdered(2)
b.first()

3)c = sc.parallelize([(3,6),(3,7),(5,8),(3,"Dog")])
d=c.countByKey()
dict(d)
c.countByValue().items()

4)t = sc.parallelize([(1, 2), (3, 4), (3, 6)])
y = t.reduceByKey(lambda x, y: x + y)
y.collect()



5)t.groupByKey().mapValues(list).collect()

6)a = t.mapValues(lambda x: x+1).collect()
b= dict(a)

c= t.map(lambda kv: (kv[0],kv[1]+1)).collect()
d=dict(c)

7)t.flatMapValues(lambda x : range(x,10)).collect()

8)t.keys().collect()

9)t.values().collect()

10)t.sortByKey().collect()



Persistence.

import pyspark as pys
b = sc.parallelize([1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1])
b.getStorageLevel()
b.persist(py.StorageLevel.DISK_ONLY)  
b.unpersist()
b.getStorageLevel()
b.persist(pys.StorageLevel.MEMORY_ONLY) 
b.getStorageLevel()

exit()

https://www.javatpoint.com/pyspark-storagelevel


